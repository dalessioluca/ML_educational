{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is missing is that the GAussian kernel is not positive definite.\n",
    "## Read reference 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinantal Point Processes (DPP) for physicist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief introduction to DPP. The best references are:\n",
    "1. https://arxiv.org/abs/1207.6083\n",
    "2. https://homes.cs.washington.edu/~taskar/pubs/dpp_tut.pdf\n",
    "3. https://arxiv.org/pdf/1205.4818.pdf (for Gaussian DPP sections 3.3 and 3.4)\n",
    "\n",
    "The main idea is that DPP are an elegant way to include repulsion between elements. The only thing you need is to specify the **similarity symmetric matrix $L$** where $L$ is positive semidefinite, i.e. **all its eigenvalues are non-negative**. A related object is is the **marginal kernel $K$**. The relation between the two is:\n",
    "\n",
    "\\begin{equation}\n",
    "L=K(I-K)^{-1},\\quad K=L(L+I)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "In terms of the spectral decompisition the relation above simply means that:\n",
    "\n",
    "\\begin{equation}\n",
    "L=\\sum_{i=1}^N \\lambda_i v_i v_i^T,\\quad K=\\sum_{i=1}^N \\frac{\\lambda_i}{1+\\lambda_i} v_i v_i^T\n",
    "\\end{equation}\n",
    "\n",
    "The similarity matrix $L$ has size $N \\times N$. For example, if there are $N$ locations on a line the matrix $L$ gives the similarity between any to pair of locations. To achieve particle repulsion we assume that $L_{i,i}=1$ and $L_{i,j}$ decays with the cartesian distance between locations $i$ and $j$. A common choice is to use $L_{i,j}=v_i \\cdot v_j$ where $v_i, v_j$ are the vectors corresponding to elements $i,j$ respectively. This means that $L=B^T B$ matrix where $B^T$ is tall and $B$ is wide. In this case $L$ is known as a Grahm matrix and for any vector $z$ we have $z^T L z= z^T B^T B z = (Bz)^T (Bz) = |Bz|^2 \\ge 0$, i.e. $L$ is positive definite.\n",
    "\n",
    "Other choices are possible but you need to make sure that the resulting matrix is positive definite.\n",
    "Another choice is a gaussian kernel: $L_{i,j}=\\exp[-\\gamma d(i,j)^2]$. This matrix is positive semi-definite. \n",
    "\n",
    "The fundamental properties of a DPP are:\n",
    "1. The probability of selecting any subset $Y$ of particles is $P(Y) = \\frac{det(L_Y)}{det(L+I)}$ where $L_Y$ is the similarity matrix restricted to the rows and columns corresponding to the few particles selected and $det(L+I)$ is just the normalization constant. Therefore the probability of each selecting the subset $Y=(i_1,i_3,i_7)$ is $P(Y)\\propto det(L_Y)$ where $L_Y$ is a $3 \\times 3$ matrix. \n",
    "2. The normalization constant is $\\sum_Y det(L_Y) = det(L+I)$. For example if there are only two locations then there are only 4 possibility: zero particle, one particle at 1, one particle at 2, two particles at 1 and 2. Then we have\n",
    "\\begin{equation}\n",
    "\\sum_Y det(L_Y)= 1 + L_{1,1} + L_{2,2} + \\left(L_{1,1}L_{2,2} - L_{1,2}L_{2,1}\\right) = (L_{1,1}+1)(L_{2,2}+1)- L_{1,2}L_{2,1} = det(L+I).\n",
    "\\end{equation} Note that we have adopted the convention that the determinant of the empy matrix is $1$, i.e. $det(L_{Y=\\text{empty}})=1$. \n",
    "3. Sometimes we are interested in the probability of inclusion, i.e. we are interested in the probability that particles $A=(i_1,i_2)$ belong to the subset $Y$. This probability is $P(A\\subseteq Y)\\propto det(K_A)$ where $K_A$ is the marginal kernel restricted to the rows and columns corresponding to the subset $A$.\n",
    "4. DPP encodes repulsion because the probability of selecting the particle at location $i$ is $P(i\\subseteq Y) = K_{i,i}$ and the probability of selecting two particles at locations $i,j$ is $P(i,j\\subseteq Y) = L_{i,i}L_{j,j}-L_{i,j}L_{j,i} = P(i)P(j) - L_{i,j}^2 \\le P(i)P(j)$, i.e. the particle repeal each other. \n",
    "\n",
    "**Note that DPP are probability distribution over all possible subsets of $N$ elements. Obviously the number of such subsets scales exponentially in $N$ but, amazingly, the normalization constant can be computed in $\\mathcal{O}(N^3)$ operations (any determinant can be computed by LU-factorization).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from DPP intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very nice property of DPP is that an efficient sampling algorithm exists. \n",
    "\n",
    "We can use physics to provide the intuition. Suppose that you consider a system of **non-interacting fermions** with Hamiltonian $H$. In this scenario $H_{i,i}$ describes the external potential at site $i$ and $H_{i,j}$ describes the hopping between site $i$ and $j$ (the hopping is long range but decays with distance). Now you diagonalize $H$ to obtain its eigenvalues and eigenvectors $\\lambda_i,v_i$. \n",
    "In physics any configuration (i.e. wave-function) of $n$ **non-interacting fermions** can be obtained by filling $n$ single-particle eigenstates and by **anti-symmetrize**. The total energy of the configuration is $E=\\sum \\lambda_i$ and the probability of that configuration is $p\\propto exp[-\\beta E] = exp[-\\beta \\sum_i \\lambda_i ] = \\prod_i f(\\lambda_i)$ where $f(\\lambda)\\equiv \\exp[-\\beta \\lambda]$. \n",
    "\n",
    "The idea is the same for DPP with the difference that $H=-K$ and that the function $f(\\lambda)$ is not an exponentail but it is linear, i.e. $f(\\lambda)\\equiv\\lambda$. This is necessary since in the DPP the probability is proportional to the determinant which is just the product of the eigenvalues. \n",
    "\n",
    "So if we want to sample the location of particles according to a DPP process we should:\n",
    "1. select each eigenstate of $K$ with probability equal to its eigenvalues (note that the eigenvalues are $\\in (0,1)$ b/c $K$ is positive definite).\n",
    "2. sample the location of the particles according to that anti-symmetrized wave-function.\n",
    "\n",
    "In the math literature, this approach is based on the fact that **DPPs are Mixtures of Elementary DPPs**. In step (1) we select the mixture (of eigenstate) and in step (2) we sample from that mixture without worrying about the values of the selected eigenvalues.\n",
    "Step (1) is trivial. Step (2) on the other hand is still difficul.\n",
    "\n",
    "For example, let us say, that you have selected the eigenstates $v_1,v_5,v_7$. Now you have to sample the location x,y,z of these three particles correspong to the wave-function \n",
    "\\begin{equation}\n",
    "\\psi(x,y,z)= \\frac{1}{\\sqrt 3} det \\begin{bmatrix}v_1(x) & v_1(y) & v_1(z) \\\\v_5(x) & v_5(y) & v_5(z) \\\\ v_7(x) & v_7(y) & v_7(z) \\end{bmatrix}\n",
    "\\end{equation}\n",
    "This is difficult because we do NOT want to work with the full wave-function. If there are 100 locations on the line to choose from the three-particles wave-function above has $100^3=10^6$ distinct values. The brute force approach would be to use the three selected eigenstates to compute the $10^6$ entries for the matrix $M_{i,j,k}$ and then select one entry proportionally to its value. The selected entry would specify, at once, the locations of the three particles. **This, all at one approach, scales badly with the number $n$ of particles** (the total wave-function has $N^n$ values). We would much rather **sample one particle at the time and then update the wave-function to take into account the previous choices**.\n",
    "\n",
    "Again, the intuition comes from physics. Having observed one particle at location $x_0$ we know that no other particles can be at location $x_0$ (because the total wave-function vanishes every time two corrdinates are intentical). Therefore we need to take the wave-function and project it on the subspace perpendicular to $\\psi(x_0)$. We can now repeat the process, i.e. sample another particle and so on. \n",
    "\n",
    "Mathematically, this approach relyes on:\n",
    "1. DPP are closed under conditioning (i.e. projecting a fermionic wave-function on a subspace gives another fermionin wave-function)\n",
    "2. the order of the observation and projection does not matter, i.e. sequentially sample $x_0,x_1,x_2$ is exactly as likely as sampling $x_1,x_0,x_2$ or any other permutations. \n",
    "\n",
    "<img src=\"./figure/DPP_conditioning_figure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from DPP algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that:\n",
    "1. $H=-K$\n",
    "2. the probability of selecting each eigenstate is proportional to its eigenvalue\n",
    "3. $K=L(L+I)^{-1}$ \n",
    "\n",
    "Then the algorithm for smapling from the DPP is:\n",
    "0. Define the similarity matrix $L$ and perform spectral decomposition $L=V D V^T$ where $D=\\text{diag}(\\lambda_1,\\lambda_2,...,\\lambda_N)$ and $V$ contains as columns the eigenvectors of $L$, i.e. $L V_\\alpha = \\lambda_\\alpha V_\\alpha$. Note that $L,V$ are $N \\times N$.\n",
    "2. select each $v_\\alpha$ with probability $p_\\alpha=\\frac{\\lambda_\\alpha}{1+\\lambda_\\alpha}$\n",
    "3. At this point you have selected a particular set of eigenvectors $V_J = \\{v_\\alpha : \\alpha \\in J\\}$. Now you have to sample from the elementary DPP with marginal kernel $K^J = V_J V_J^T = \\sum_{\\alpha \\in J} V_\\alpha V_\\alpha^T$ where $V_J$ is a matrix of size $|J|\\times N$, i.e. $V_J$ is tall matrix and $V_J^T$ is wide matrix\n",
    "4. Select a location $P(i\\subset Y) = K^J_{i,i} = <i| \\left(\\sum_{\\alpha \\in J} v_\\alpha v_\\alpha^T\\right)|i> = \\sum_{\\alpha \\in J} v_\\alpha(i)^2$\n",
    "4. Project the wave-function on the subspace orthogonal to $e_i$ and compute an orto-normal basis in that space. Go back to step 3.  \n",
    "\n",
    "See figure below for the schematic of the algorithm\n",
    "<img src=\"./figure/sample_DPP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-DPP\n",
    "\n",
    "Suppose we want exactly $k$ particles. The probability of k-DPP is\n",
    "\n",
    "\\begin{equation}\n",
    "P_L^k(Y)=\\frac{P_L(Y)}{\\sum_{|Y'|=k}P_L(Y')}=\\frac{det(L_Y)}{\\sum_{|Y'|=k}det(L_{Y'})}\n",
    "\\end{equation}\n",
    "Note that the normalization constant is now much more difficult to compute, i.e. it is no longer the determinant of $det(L+I)$.  \n",
    "However, it is clear that, \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{|Y'|=2} det(L_{Y'}) = \\lambda_1\\lambda_2 + \\lambda_1 \\lambda_3 + \\dots + \\lambda_{N-1}\\lambda_N\n",
    "\\end{equation}\n",
    "i.e. the sum of all the distinct polynomials of degree $2$ that can be generated using $\\Lambda=\\{\\lambda_1,\\dots,\\lambda_N\\}$. More generally:\n",
    "\n",
    "\\begin{equation}\n",
    "Z_k = \\sum_{|Y'|=k} det(L_{Y'}) = e_k(\\lambda_1,\\dots,\\lambda_N)\n",
    "\\end{equation}\n",
    "where $e_k$ is the elementary symmetric polynomial.\n",
    "Likely the symmetric polynomials can be computed recursively:\n",
    "\n",
    "See figure below for the recursion algorithm\n",
    "<img src=\"./figure/symmetric_poly.png\">\n",
    "\n",
    "Once the symmetric polynomials are known we can sample the k eigenvectors according to the algorithm below:\n",
    "<img src=\"./figure/sample_k_eigenvectors.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class dpp():\n",
    "    def __init__(self):\n",
    "        self.lenght = None \n",
    "        self.D  = None \n",
    "        self.V  = None\n",
    "        self.M  = None #It is important that the matrix is positive definite\n",
    "        self.E  = None \n",
    "    \n",
    "    def compute_symmetric_poly(self,k: int) -> None:\n",
    "        N=self.lenght\n",
    "        self.E=np.zeros((N+1,k+1)) # i.e. E[N,k] is IN bound\n",
    "        self.E[:,0]=1\n",
    "        for l in range(1,k+1):\n",
    "            for n in range(1,N+1):\n",
    "                self.E[n,l]=self.E[n-1,l]+self.D[n-1]*self.E[n-1,l-1]\n",
    "        \n",
    "    def select_k_eigenvectors(self,k: int) -> list:\n",
    "         \n",
    "        # preparation\n",
    "        N=self.lenght\n",
    "        n=N\n",
    "        l=k\n",
    "        selected = []\n",
    "        \n",
    "        # Compute the symmetric polynomial if I don;t have them already\n",
    "        if(self.E is None):\n",
    "            self.compute_symmetric_poly(k)\n",
    "        elif(k != self.E.shape[1]):\n",
    "            self.compute_symmetric_poly(k)\n",
    "        \n",
    "        while l > 0:\n",
    "            marg = self.D[n-1]*self.E[n-1,l-1]/self.E[n,l]\n",
    "            if(np.random.uniform() < marg):\n",
    "                selected.append(n-1) # the -1 is just an indexing problems\n",
    "                l = l - 1\n",
    "            n = n - 1\n",
    "        \n",
    "        #print(\"selected eigenvectors\",selected)\n",
    "        return selected\n",
    "\n",
    "            \n",
    "    \n",
    "    def sample(self,k=None,verbose=False) -> list:\n",
    "        \"\"\"\n",
    "        Sample the DPP and returns the locations of the DPP particles.\n",
    "        If k is provided exactly k locations are returned\n",
    "        \"\"\"\n",
    "            \n",
    "        # PHASE 1: select the eigenvectors\n",
    "        if(k==None):\n",
    "            # Select each eigenvectors with probability lambda/(1+lambda)\n",
    "            prob = np.divide(self.D,1+self.D)\n",
    "            x = np.random.rand(prob.size)\n",
    "            v = np.argwhere(x<prob)\n",
    "            k = len(v)    \n",
    "            V = self.V[:, v]\n",
    "            V = V.reshape(self.lenght,-1)\n",
    "        elif(k==self.lenght):\n",
    "            #select all the eigenvectors\n",
    "            k=self.lenght\n",
    "            V=self.V\n",
    "        elif(k>self.lenght):\n",
    "            # error: Cannot sample more than self.lenght DPP particles\n",
    "            print(\"Error: k>self.lenght\")\n",
    "            sys.exit()\n",
    "        else:\n",
    "            # select exactly k eigevectors\n",
    "            selected=self.select_k_eigenvectors(k)\n",
    "            k = len(selected)    \n",
    "            V = self.V[:, selected]\n",
    "            V = V.reshape(self.lenght,-1)\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"I will select \"+str(k)+\" points:\")\n",
    "            \n",
    "            \n",
    "        # PHASE 2: Select iteratively a location\n",
    "        #ground_set, rem_set = np.arange(N), np.full(N, True)\n",
    "        locations = list(np.arange(0,self.lenght))\n",
    "        chosen=[]\n",
    "    \n",
    "        for n in range(0,k):\n",
    "            # compute probabilities for each item\n",
    "            # N, k = V.shape \n",
    "            prob=np.linalg.norm(V,ord=None,axis=1)**2/(k-n) #this is probability of choosing each location\n",
    "            prob=prob.flatten()\n",
    "            \n",
    "            # select one location from the available_one \n",
    "            loc = np.random.choice(locations,size=1, p=prob.flatten()).item()\n",
    "            chosen.append(loc)\n",
    "            if(verbose):\n",
    "                print(\"add location\",loc)\n",
    "            \n",
    "            if(len(chosen)==k):\n",
    "                return chosen\n",
    "            else:\n",
    "                \n",
    "                # V is a ortonormal basis.\n",
    "                # Now I need to find a new ortonormal basis for the subspace of V perpendicular to e_loc\n",
    "                # The constraint that the subspace spanned by V is hortogonal to e_loc is:\n",
    "                # (c1 V_1 + c2 V_2 + c_k V_k) cdot e_loc = 0 \n",
    "                # This is equivalent to:\n",
    "                # c1 V_1(loc) + c2 V_2(loc) + c_k V_k(loc) = 0\n",
    "                # This subspace has dimension k-1 since the coefficients c1,...,ck are now constrained.\n",
    "                # I need to find a new ortonormal basis for this subspace.\n",
    "                # \n",
    "                # The procedure is as follow:\n",
    "                # e_loc = c1* V_1 + c2* V_2 + ... + ck* V_k \n",
    "                # The coeff of this decomposition are already available (read off the entry of the vectors):\n",
    "                # cn* = <e_loc,V_n> = V_n(loc)\n",
    "                # Build k-1 vectors in the subspace of V perpendicular to e_loc by doing:\n",
    "                # U_n = V_n - V_1 (cn*/c1*) for n=2,k\n",
    "                # Note that: <U_n | e_loc> = cn* - c1* (cn*/c1*) = 0\n",
    "                # Feed these k-1 vectors into a Gram–Schmidt procedure to obtain a hortonormal basis\n",
    "                # The procedure above works only is c1* is different from zero.\n",
    "                # For this reason I save myself some problem by choosing V1 as the vector with the largest coeff.\n",
    "                # Recall that:\n",
    "                # N, k = V.shape\n",
    "                \n",
    "                cstar=V[loc,:] #for each one of the k eigenvectors I have a cstar \n",
    "                j  = np.argmax(cstar)\n",
    "                Vj = V[:,j] # select the special vector\n",
    "                cj = Vj[loc]\n",
    "                cstar = np.delete(cstar,j)\n",
    "                V = np.delete(V,j,axis=1)\n",
    "                \n",
    "                # Next I do a vectorized form for:\n",
    "                # U[0:N,0:k-1] = V[0:N,0:k-1] - Vj[0:N] (cstar[0:k-1]/cj)\n",
    "                V=V-np.matmul(Vj.reshape(-1,1),(cstar/cj).reshape(1,-1))\n",
    "                     \n",
    "                # Gram–Schmidt procedure to find a basis out of the representative vectors\n",
    "                basis = []\n",
    "                for l in range(k-n-1):\n",
    "                    v=V[:,l]\n",
    "                    w = v - np.sum( np.dot(v,b)*b  for b in basis )\n",
    "                    basis.append(w/np.linalg.norm(w))\n",
    "                V=np.array(basis).T\n",
    "                if(verbose):\n",
    "                    print(\"len(basis)\",len(basis))\n",
    "        \n",
    "        \n",
    "class dpp_line(dpp):\n",
    "    def __init__(self, lenght=10, sigma2=1, similarity='soft', bc='PBC'):\n",
    "        super().__init__()\n",
    "        self.lenght = lenght\n",
    "        data = np.zeros([lenght,1])\n",
    "        data[:,0]=np.arange(0,lenght)\n",
    "        self.M= np.zeros((self.lenght,self.lenght))\n",
    "        gamma = 1.0/(2.0*sigma2)\n",
    "        \n",
    "        # Compute the matrix with distance-square\n",
    "        for n1 in range(0,self.lenght):\n",
    "            self.M[n1,n1]= 1.0\n",
    "            for n2 in range(n1+1,self.lenght):\n",
    "                if(bc == 'OBC'):\n",
    "                    dx = data[n1,0]-data[n2,0]\n",
    "                elif(bc == 'PBC'):\n",
    "                    dx =(data[n1,0]-data[n2,0]+self.lenght ) % self.lenght\n",
    "                    dx = min(dx, self.lenght - dx)\n",
    "                else:\n",
    "                    print(\"ERROR: Unknown bc\")\n",
    "                    sys.exit()\n",
    "                \n",
    "                d2=dx*dx\n",
    "                \n",
    "                if(similarity == 'soft'):\n",
    "                    tmp = np.exp(-gamma*d2)    \n",
    "                elif(similarity == 'hard'):\n",
    "                    tmp = 0.5*float(d2<sigma2)\n",
    "                else:\n",
    "                    print(\"ERROR: Unknown similarity\")\n",
    "                    sys.exit()\n",
    "                    \n",
    "                self.M[n1,n2]= tmp\n",
    "                self.M[n2,n1]= tmp\n",
    "        \n",
    "        # Diagonalize matrix\n",
    "        self.D, self.V  = np.linalg.eigh(self.M) # Use this because matrix is symmetric\n",
    "        for l in self.D:\n",
    "            if(l<-1E-10):\n",
    "                print(\"ERROR: Matrix is NOT positive definite\")\n",
    "                print(self.D)\n",
    "                print(self.M)\n",
    "                sys.exit()\n",
    "        \n",
    "    def sample(self,k=None,verbose=False):\n",
    "        chosen = np.array(super().sample(k,verbose))\n",
    "        return chosen\n",
    "\n",
    "class dpp_plane(dpp):\n",
    "    def __init__(self, lx=10, ly=10, sigma2=1, similarity='soft', bc='PBC'):\n",
    "        super().__init__()\n",
    "        \n",
    "        data = np.zeros((lx,ly,2))\n",
    "        for ix in range(lx):\n",
    "            data[ix,:,0]=ix\n",
    "        for iy in range(ly):\n",
    "            data[:,iy,1]=iy\n",
    "        data=data.reshape(-1,2)\n",
    "        self.data=data\n",
    "        self.lenght = lx*ly\n",
    "        self.lx = lx\n",
    "        self.ly = ly\n",
    "        self.M= np.zeros((self.lenght,self.lenght))\n",
    "        gamma = 1.0/(2.0*sigma2)\n",
    "        \n",
    "        # Compute the matrix with distance-square\n",
    "        for n1 in range(0,self.lenght):\n",
    "            self.M[n1,n1]= 1.0\n",
    "            for n2 in range(n1+1,self.lenght):\n",
    "                if(bc == 'OBC'):\n",
    "                    dx = data[n1,0]-data[n2,0]\n",
    "                    dy = data[n1,1]-data[n2,1]\n",
    "                elif(bc == 'PBC'):\n",
    "                    dx =(data[n1,0]-data[n2,0]+self.lx ) % self.lx\n",
    "                    dx = min(dx, self.lx - dx)\n",
    "                    dy =(data[n1,1]-data[n2,1]+self.ly ) % self.ly\n",
    "                    dy = min(dy, self.ly - dy)\n",
    "                else:\n",
    "                    print(\"ERROR: Unknown bc\")\n",
    "                    sys.exit()\n",
    "                \n",
    "                d2=dx*dx+dy*dy\n",
    "                \n",
    "                if(similarity == 'soft'):\n",
    "                    tmp = np.exp(-gamma*d2)    \n",
    "                elif(similarity == 'hard'):\n",
    "                    tmp = 0.5*float(d2<sigma2)\n",
    "                else:\n",
    "                    print(\"ERROR: Unknown similarity\")\n",
    "                    sys.exit()\n",
    "                    \n",
    "                self.M[n1,n2]= tmp\n",
    "                self.M[n2,n1]= tmp\n",
    "        \n",
    "        # Diagonalize matrix\n",
    "        self.D, self.V  = np.linalg.eigh(self.M) # Use this because matrix is symmetric\n",
    "        for l in self.D:\n",
    "            if(l<-1E-10):\n",
    "                print(\"ERROR: Matrix is NOT positive definite\")\n",
    "                print(self.D)\n",
    "                print(self.M)\n",
    "                sys.exit()\n",
    "        \n",
    "    def sample(self,k=None,verbose=False):\n",
    "        chosen_list = super().sample(k,verbose)\n",
    "        xy_pair=np.array(self.data[chosen_list])\n",
    "        return xy_pair\n",
    "        \n",
    "    \n",
    "class random():\n",
    "    def __init__(self):\n",
    "        self.lenght = None\n",
    "    \n",
    "    def sample(self,k=None) -> list:\n",
    "        \"\"\"\n",
    "        Sample k random locations without replacement\n",
    "        A list of locations is returned\n",
    "        \"\"\"\n",
    "        chosen=np.random.choice(range(self.lenght),size=k,p=None).tolist()\n",
    "        return chosen\n",
    "\n",
    "class random_line(random):\n",
    "    def __init__(self, lenght=10):\n",
    "        super().__init__()\n",
    "        self.lenght = lenght\n",
    "    def sample(self,k=5):\n",
    "        chosen = np.array(super().sample(k))\n",
    "        return chosen\n",
    "    \n",
    "class random_plane(random):\n",
    "    def __init__(self, lx=10, ly=10):\n",
    "        super().__init__()\n",
    "        data = np.zeros((lx,ly,2))\n",
    "        for ix in range(lx):\n",
    "            data[ix,:,0]=ix\n",
    "        for iy in range(ly):\n",
    "            data[:,iy,1]=iy\n",
    "        data=data.reshape(-1,2)\n",
    "        self.data=data\n",
    "        self.lenght = lx*ly\n",
    "    \n",
    "    def sample(self,k=10):\n",
    "        chosen_list = super().sample(k)\n",
    "        xy_pair=np.array(self.data[chosen_list])\n",
    "        return xy_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17  6  1 15  2 14  0  8 12 10 19  4]\n"
     ]
    }
   ],
   "source": [
    "dpp=dpp_line(lenght=20,sigma2=2, similarity='soft', bc='PBC')\n",
    "xy_dpp=dpp.sample(verbose=False,k=12)\n",
    "print(xy_dpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = 10\n",
    "ly = 10\n",
    "dpp=dpp_plane(lx=lx,ly=ly,sigma2=1, similarity='hard',bc='OBC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_points: dpp vs random 12 12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xy_dpp=dpp.sample(verbose=False,k=12)\n",
    "n_points_dpp = xy_dpp.shape[0]\n",
    "random2=random_plane(lx,ly)\n",
    "xy_random=random2.sample(n_points_dpp)\n",
    "n_points_random = xy_random.shape[0]\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, subplot_kw = {'aspect' : 1.0})\n",
    "ax1.scatter(xy_dpp[:,0],xy_dpp[:,1])\n",
    "ax1.set_title('DPP')\n",
    "ax2.scatter(xy_random[:,0],xy_random[:,1])\n",
    "ax2.set_title('random without replacement')\n",
    "print(\"N_points: dpp vs random\",n_points_dpp,n_points_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyro]",
   "language": "python",
   "name": "conda-env-pyro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
