{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinantal Point Processes (DPP) for physicist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief introduction to DPP. We are interested in practical application and therefore we focus on the so-called $L_{ensamble}$. \n",
    "\n",
    "The main idea is that DPP are an elegant way to include repulsion between elements. The only thing you need is to specify the **similarity symmetric matrix $L$** where $L$ is positive semidefinite, i.e. **all its eigenvalues are non-negative**.\n",
    "1. The similarity matrix $L$ has size $N \\times N$. For example, if there are $N$ locations on a line the matrix $L$ gives the similarity between any to pair of locations. To achieve particle repulsion we assume that $L_{i,i}=1$ and $L_{i,j}$ decays with the cartesian distance between locations $i$ and $j$. A common choice is to use a gaussian kernel: $L_{i,j}=\\exp[-\\gamma d(i,j)^2]$. This matrix is positive semi-definite. Another common choice is $L_{i,j}=v_i \\cdot v_j$ where $v_i, v_j$ are the vectors corresponding to elements $i,j$ respectively.\n",
    "2. The fundamental property of DPP is that the probability of selecting any subset $Y$ of particles is $P(Y)\\propto det(L_Y)$ where $L_Y$ is the similarity matrix restricted to the rows and columns corresponding to the few particles selected. Neglecting the normalizing constant for a second we see that the probability of selecting just one particle at location $i$ is $P(i) = L_{i,i}$ and the probability of selecting only two particles at locations $i,j$ is $P(i,j) = L_{i,i}L_{j,j}-L_{i,j}L_{j,i} = P(i)P(j) - L_{i,j}^2 \\le P(i)P(j)$, i.e. the particle repeal each other. \n",
    "4. The normalization constant is $\\sum_Y det(L_Y) = det(L+I)$. For example if there are only two locations then there are only 4 possibility: zero particle, one particle at 1, one particle at 2, two particles at 1 and 2. Then we have\n",
    "\\begin{equation}\n",
    "\\sum_Y det(L_Y)= 1 + L_{1,1} + L_{2,2} + \\left(L_{1,1}L_{2,2} - L_{1,2}L_{2,1}\\right) = (L_{1,1}+1)(L_{2,2}+1)- L_{1,2}L_{2,1} = det(L+I).\n",
    "\\end{equation}\n",
    "Note that we have adopted the convention that the determinant of the empy matrix is $1$, i.e. $det(L_{Y=\\text{empty}})=1$. \n",
    "\n",
    "Note that DPP are probability distribution over all possible subsets of $N$ elements. Obviously the number of such subsets scales exponentially in $N$ but, amazingly, the normalization constant can be computed in $\\mathcal{O}(N^3)$ operations (any determinant can be computed by LU-factorization). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from DPP intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very nice property of DPP is that an efficient sampling algorithm exists. \n",
    "\n",
    "We can use physics to provide the intuition. Suppose that you consider a system of **non-interacting fermions** with Hamiltonian $H=-L$. In this scenario $-L_{i,i}$ describes the external potential at site $i$ and $-L_{i,j}$ describes the hopping between site $i$ and $j$. Now you diagonalize $H$ to obtain its eigenvalues and eigenvectors $\\lambda_i,v_i$. \n",
    "In physics any configuration (i.e. wave-function) of $n$ **non-interacting fermions** can be obtained by filling $n$ single-particle eigenstates and by **anti-symmetrize**. The total energy of the configuration is $E=\\sum \\lambda_i$ and the probability of that configuration is $p\\propto exp[-\\beta E] = exp[-\\beta \\sum_i \\lambda_i ] = \\prod_i f(\\lambda_i)$ where $f(\\lambda)\\equiv \\exp[-\\beta \\lambda]$. \n",
    "\n",
    "The idea is the same for DPP with the difference that the function $f(\\lambda)$ is not an exponentail but it is linear, i.e. $f(\\lambda)\\equiv\\lambda$. This is necessary since in the DPP the probability is proportional to the determinant which is just the product of the eigenvalues. \n",
    "\n",
    "So if we want to sample the location of particles according to a DPP process we should:\n",
    "1. select a subset of $n$ eigenstate with probability $p\\propto \\prod \\lambda_i $ \n",
    "2. sample the location of the particles according to that anti-symmetrized wave-function.\n",
    "In the math literature, this approach is based on the fact that **DPPs are Mixtures of Elementary DPPs**. In step (1) we select the mixture (of eigenstate) and in step (2) we sample from that mixture without worrying about the values of the selected eigenvalues.\n",
    "\n",
    "For example, let us say, that you selected the eigenstates $v_1,v_5,v_7$ with probability $p\\propto \\lambda_1 \\times \\lambda_5 \\times \\lambda_7$. Now you have to sample the location x,y,z of these three particles correspong to the wave-function \n",
    "\\begin{equation}\n",
    "\\psi(x,y,z)= \\frac{1}{\\sqrt 3} det \\begin{bmatrix}v_1(x) & v_1(y) & v_1(z) \\\\v_5(x) & v_5(y) & v_5(z) \\\\ v_7(x) & v_7(y) & v_7(z) \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Step (1) is trivial. Step (2) on the other hand is still difficult because we do NOT want to work with the full wave-function. If there are 100 locations on the line to choose from the three-particles wave-function above has $100^3=10^6$ distinct values. The brute force approach would be to use the three selected eigenstates to compute the $10^6$ entries for the matrix $M_{i,j,k}$ and then select one entry proportionally to its value. The selected entry would specify, at once, the locations of the three particles. **This, all at one approach, scales badly with the number $n$ of particles** (the total wave-function has $N^n$ values). We would much rather **sample one particle at the time and then update the wave-function to take into account the previous choices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Elementary DPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that we have spectrally decomposed $L$ into $L=V D V^T$ where $D=\\text{diag}(\\lambda_1,\\lambda_2,...,\\lambda_L)$ and $V$ **contains as columns the eigenvectors of $L$**, i.e. $L V_\\alpha = \\lambda_\\alpha V_\\alpha$. Note that $L,V$ are $N \\times N$ and the determinant of $det(L)=\\prod_i^N \\lambda_i$. \n",
    "\n",
    "In step (1) (see above) you have selected a particular set of eigenvectors $V_J = \\{v_n : n \\in J\\}$. Now you have to sample from the elementary DPP with similarity matrix $L_J = V_J V_J^T$ where $V_J$ is a matrix of size $|J|\\times N$, i.e. $V_J$ is tall matrix and $V_J^T$ is wide matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{J} = V_J V_J^T = \\sum_{n \\in J} v_n v_n^T\n",
    "\\end{equation}\n",
    "Note that $L_{J}$ is a $N \\times N$ **square similarity matrix which defines a valid DPP**.\n",
    "\n",
    "As we have argued at the beginning the fundamental property of the DPP is that $P(i)=det(L_{i,i})=L_{i,i}$. From the spectal decomposition of $L_J$ we see that we can choose the location of the first particle $i$ with probability:\n",
    "\n",
    "\\begin{equation}\n",
    "P(i)=(L_{J})_{i,i} = e_i^T L_{J} e_i = \\sum_{n \\in J} (v_n\\cdot e_i)^2 =\\sum_{n \\in J} v_n(i)^2\n",
    "\\end{equation}\n",
    "where $e_i$ is the column vector with all-zeros except one 1 at the location $i$. \n",
    "Now we have to condition the DPP given the fact that we have chosen location $i$ for the first particle. Lukily DPP are closed under conditioning, i.e. after conditioning you get a new DPP. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note that \n",
    "\n",
    "\n",
    "$ So we can choose the first particle location by according to $L_{J}so we can choose a location $i . \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The matrix $L_Y$ for $Y=(1,2,7,8)$ can be obtained by **dotting** $L$ with a column and row vector which has all zero entry except ones at location 1,2,7,8. The determinant  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "probability of each subset of eigenstates is If you want a configuration with $n$ particles you can select $n$ eigenvalues \n",
    "\n",
    "\n",
    "to fit 3 particles you select 3 eigenvalues and se\n",
    "\n",
    "Then all configuration of particles can be thought as the possibility of fitting the particles into the eigenstaate.\n",
    "\n",
    "\n",
    "\n",
    "4. More generally the probability of selecting any subset $\\{i_1,i_2,...,i_N\\}$ is $det(K_N)$ where $K_N$ is the square NxN matrix obtained by selecting the N rows and columns corresponding to the desired locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the algorithm to sample a DPP\n",
    "\n",
    "<img src=\"./diagram/sample_DPP.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class dpp():\n",
    "    def __init__(self):\n",
    "        self.lenght = None \n",
    "        self.D  = None \n",
    "        self.V  = None\n",
    "    \n",
    "    def sample(self,k=None,verbose=False) -> list:\n",
    "        \"\"\"\n",
    "        Sample the DPP and returns the locations of the DPP particles.\n",
    "        If k is provided exactly k locations are returned\n",
    "        \"\"\"\n",
    "            \n",
    "        # PHASE 1: select the eigenvectors\n",
    "        if(k==None):\n",
    "            # Select each eigenvectors with probability lambda/(1+lambda)\n",
    "            prob = np.divide(self.D,1+self.D)\n",
    "            x = np.random.rand(prob.size)\n",
    "            v = np.argwhere(x<prob)\n",
    "            k = len(v)    \n",
    "            V = self.V[:, v]\n",
    "            V = V.reshape(self.lenght,-1)\n",
    "        elif(k==self.length):\n",
    "            #select all the eigenvectors\n",
    "            k=self.lenght\n",
    "            V=self.V\n",
    "        elif(k>self.lenght):\n",
    "            # error: Cannot sample more than self.lenght DPP particles\n",
    "            print(\"Error: k>self.lenght\")\n",
    "            sys.exit()\n",
    "        else:\n",
    "            # select exactly k eigenvectors\n",
    "            print(\"Error: Not implemented yet\")\n",
    "            sys.exit()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"I will select \"+str(k)+\" points:\")\n",
    "            \n",
    "            \n",
    "        # PHASE 2: Select iteratively a location\n",
    "        #ground_set, rem_set = np.arange(N), np.full(N, True)\n",
    "        locations = list(np.arange(0,self.lenght))\n",
    "        chosen=[]\n",
    "    \n",
    "        for n in range(0,k):\n",
    "            # compute probabilities for each item\n",
    "            prob=np.linalg.norm(V,ord=None,axis=1)**2/(k-n) #this is probability of choosing each location\n",
    "            prob=prob.flatten()\n",
    "            #print(\"prob\",prob)\n",
    "            \n",
    "            # select one location from the available_one \n",
    "            loc = np.random.choice(locations,size=1, p=prob.flatten()).item()\n",
    "            chosen.append(loc)\n",
    "            #print(\"add location\",loc)\n",
    "            \n",
    "            if(len(chosen)==k):\n",
    "                if(verbose):\n",
    "                    print(\"chosen points:\",chosen)\n",
    "                return chosen\n",
    "            else:\n",
    "                # project the eigenvectors of the subspace perperndicular to the chosen location\n",
    "                V[loc,:] = 0 # there is zero probability of choosing the same location twice\n",
    "                j = np.random.choice(range(k-n),size=1,p=None)\n",
    "                V = np.delete(V, j, axis=1)\n",
    "            \n",
    "                # Now I have to find a orthonormal basis in this subspace.\n",
    "                # Basically I have to do Gramâ€“Schmidt procedure\n",
    "                basis = []\n",
    "                for l in range(k-n-1):\n",
    "                    v=V[:,l]\n",
    "                    w = v - np.sum( np.dot(v,b)*b  for b in basis )\n",
    "                    basis.append(w/np.linalg.norm(w))\n",
    "                V=np.array(basis).T\n",
    "                #print(\"V.shape after norm\",V.shape)\n",
    "        \n",
    "        \n",
    "class dpp_line(dpp):\n",
    "    def __init__(self, lenght=10, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.lenght = lenght\n",
    "        data = np.zeros([lenght,1])\n",
    "        data[:,0]=np.arange(0,lenght)\n",
    "        M = rbf_kernel(data,data,gamma)\n",
    "        self.D, self.V  = np.linalg.eig(M)\n",
    "        \n",
    "    def sample(self,k=None,verbose=False):\n",
    "        chosen = super().sample(k)\n",
    "        return chosen\n",
    "\n",
    "class dpp_plane(dpp):\n",
    "    def __init__(self, lx=10, ly=10, gamma=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        data = np.zeros((lx,ly,2))\n",
    "        for ix in range(lx):\n",
    "            data[ix,:,0]=ix\n",
    "        for iy in range(ly):\n",
    "            data[:,iy,1]=iy\n",
    "        data=data.reshape(-1,2)\n",
    "        self.data=data\n",
    "        M= rbf_kernel(data,data,gamma=0.2)\n",
    "        self.lenght = lx*ly\n",
    "        self.D, self.V  = np.linalg.eig(M)\n",
    "\n",
    "        \n",
    "    def sample(self,k=None,verbose=False):\n",
    "        chosen_list = super().sample(k)\n",
    "        xy_pair=np.array(self.data[chosen_list])\n",
    "        return xy_pair\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10c080710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADL5JREFUeJzt3U+InPUdx/HPp5tIN7ESwb0kka5CSVuENjIUq+DBWNKiaA6FWtCDRXJp6x8kYrxIzxHRQxFCrBelHuISRMRY0GMRZ10h1TQg/onZRByhqyLbmui3h911k/3jPmvmye/33Xm/TmZ8HL/OM/NmfWZ2vo4IAQDy+F7pAQAAq0O4ASAZwg0AyRBuAEiGcANAMoQbAJIh3ACQDOEGgGQINwAks66NO73ssstidHS0jbsGgDVpfHz8k4gYaXJsK+EeHR1Vt9tt464BYE2y/UHTY7lUAgDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZFr5OOB3cWhiUvsOH9PJqWlt3jSsPTu3adf2LaXHAoAVXeh+VRHuQxOT2jt2RNOnv5IkTU5Na+/YEUki3gCqVqJfVVwq2Xf42Df/0XOmT3+lfYePFZoIAJop0a8qwn1yanpVtwNALUr0q4pwb940vKrbAaAWJfpVRbj37Nym4fVD59w2vH5Ie3ZuKzQRADRTol9VvDk5dwGfT5UAyKZEvxwRfb/TTqcTfDsgADRnezwiOk2OreJSCQCgOcINAMkQbgBIhnADQDKEGwCSIdwAkAzhBoBkCDcAJEO4ASAZwg0AyRBuAEiGcANAMoQbAJJp9LWutu+TdJekkHRE0p0R8d9+DsKyYCAPXq9lrfgTt+0tku6W1ImIqyQNSbqtn0PMLducnJpWaH7Z5qGJyX7+awD0Aa/X8ppeKlknadj2OkkbJJ3s5xAsCwby4PVa3orhjohJSY9IOi7plKRPI+LlhcfZ3m27a7vb6/VWNQTLgoE8eL2W1+RSyaWSbpV0haTNkjbavn3hcRGxPyI6EdEZGRlZ1RAsCwby4PVaXpNLJTdKei8iehFxWtKYpGv7OQTLgoE8eL2W1+RTJcclXWN7g6RpSTsk9XWhJMuCgTx4vZbXaFmw7b9I+p2kM5ImJN0VEf9b7niWBQPA6qxmWXCjz3FHxMOSHj6vqQAAfcFvTgJAMoQbAJIh3ACQDOEGgGQINwAkQ7gBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZAg3ACTT6NsBMZjY5A3UiXBjSXObvOeWws5t8pZEvIHCuFSCJbHJG6gX4caS2OQN1ItwY0ls8gbqRbixJDZ5A/XizUksiU3eQL0IN5a1a/sWQg1UiEslAJAM4QaAZAg3ACRDuAEgGcINAMkQbgBIhnADQDKEGwCSIdwAkAzhBoBkCDcAJEO4ASAZwg0AyRBuAEim0de62t4k6YCkqySFpD9ExD/bHAzA0g5NTPI96ZW50Oek6fdxPy7ppYj4re2LJG1obSIAyzo0Mam9Y0e+WeQ8OTWtvWNHJIl4F1LinKx4qcT2JZKul/SkJEXElxEx1co0AL7VvsPHvgnEnOnTX2nf4WOFJkKJc9LkGveVknqSnrI9YfuA7Y0LD7K923bXdrfX6/V9UADSyanpVd2O9pU4J03CvU7S1ZKeiIjtkr6Q9ODCgyJif0R0IqIzMjLS5zEBSNLmTcOruh3tK3FOmoT7hKQTEfHa7J8PaibkAC6wPTu3aXj90Dm3Da8f0p6d2wpNhBLnZMU3JyPiI9sf2t4WEcck7ZD0dmsTAVjW3JtdfKqkHiXOiSNi5YPsn2vm44AXSXpX0p0R8Z/lju90OtHtdvs2JACsdbbHI6LT5NhGHweMiDclNbpDAEC7+M1JAEiGcANAMoQbAJIh3ACQDOEGgGQINwAkQ7gBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZJouCwaKYas5cC7Cjaqx1RxYjEslqBpbzYHFCDeqxlZzYDHCjaqx1RxYjHCjamw1BxbjzUlUja3mwGKEG9XbtX0LoQbOwqUSAEiGcANAMoQbAJIh3ACQDOEGgGQINwAkQ7gBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZBqH2/aQ7QnbL7Q5EADg263ma13vkXRU0iUtzYLKsF0dy+G5UVajn7htb5V0k6QD7Y6DWsxtV5+cmlZofrv6oYnJ0qOhMJ4b5TW9VPKYpAckfd3iLKgI29WxHJ4b5a0Ybts3S/o4IsZXOG637a7tbq/X69uAKIPt6lgOz43ymvzEfZ2kW2y/L+lZSTfYfnrhQRGxPyI6EdEZGRnp85i40NiujuXw3ChvxXBHxN6I2BoRo5Juk/RKRNze+mQoiu3qWA7PjfJYFowlsV0dy+G5UZ4jou932ul0otvt9v1+AWCtsj0eEZ0mx/KbkwCQDOEGgGQINwAkQ7gBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZAg3ACRDuAEgGcINAMkQbgBIhu/jrhRbtOvDOUEtCHeF5rZozy1knduiLYlQFMI5QU24VFIhtmjXh3OCmhDuCrFFuz6cE9SEcFeILdr14ZygJoS7QmzRrg/nBDXhzckKsUW7PpwT1IQt7wBQAba8A8AaRrgBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZAg3ACRDuAEgGcINAMkQbgBIhnADQDIrhtv25bZftX3U9lu277kQgwEAltbk+7jPSLo/It6w/QNJ47b/ERFvtzwbIInt6jXinJS1Yrgj4pSkU7N//bnto5K2SCLcaB3b1evDOSlvVde4bY9K2i7ptTaGARZiu3p9OCflNQ637YslPSfp3oj4bIm/v9t213a31+v1c0YMMLar14dzUl6jcNter5loPxMRY0sdExH7I6ITEZ2RkZF+zogBxnb1+nBOymvyqRJLelLS0Yh4tP2RgHlsV68P56S8Jp8quU7SHZKO2H5z9raHIuLF9sYCZrBdvT6ck/LY8g4AFWDLOwCsYYQbAJIh3ACQDOEGgGQINwAkQ7gBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZAg3ACRDuAEgmSbfxw0AVRrUbfOEG0BKg7xtnkslAFIa5G3zhBtASoO8bZ5wA0hpkLfNE24AKQ3ytnnenASQ0iBvmyfcANLatX3LQIR6IS6VAEAyhBsAkiHcAJAM4QaAZAg3ACRDuAEgGcINAMkQbgBIhnADQDKEGwCSIdwAkAzhBoBkCDcAJNMo3LZ/bfuY7XdsP9j2UACA5a34ta62hyT9VdKvJJ2Q9Lrt5yPi7baHQ1m1bNBmDuBcTb6P+xeS3omIdyXJ9rOSbpVEuNewWjZoMwewWJNLJVskfXjWn0/M3oY1rJYN2swBLNYk3F7itlh0kL3bdtd2t9frnf9kKKqWDdrMASzWJNwnJF1+1p+3Sjq58KCI2B8RnYjojIyM9Gs+FFLLBm3mABZrEu7XJf3I9hW2L5J0m6Tn2x0LpdWyQZs5gMVWfHMyIs7Y/pOkw5KGJP0tIt5qfTIUVcsGbeYAFnPEosvV563T6US32+37/QLAWmV7PCI6TY7lNycBIBnCDQDJEG4ASIZwA0AyhBsAkmnlUyW2e5I++I7/+GWSPunjONnxeMzjsTgXj8e8tfBY/DAiGv32YivhPh+2u00/EjMIeDzm8Vici8dj3qA9FlwqAYBkCDcAJFNjuPeXHqAyPB7zeCzOxeMxb6Aei+qucQMAvl2NP3EDAL5FVeFmKfEM25fbftX2Udtv2b6n9Eyl2R6yPWH7hdKzlGZ7k+2Dtv89+xz5ZemZSrJ93+zr5F+2/277+6Vnals14T5rKfFvJP1U0u9t/7TsVMWckXR/RPxE0jWS/jjAj8WceyQdLT1EJR6X9FJE/FjSzzTAj4vtLZLultSJiKs089XTt5Wdqn3VhFtnLSWOiC8lzS0lHjgRcSoi3pj9688188Ic2C9+tr1V0k2SDpSepTTbl0i6XtKTkhQRX0bEVNmpilsnadj2OkkbtMSGrrWmpnCzlHgJtkclbZf0WtlJinpM0gOSvi49SAWulNST9NTspaMDtjeWHqqUiJiU9Iik45JOSfo0Il4uO1X7agp3o6XEg8T2xZKek3RvRHxWep4SbN8s6eOIGC89SyXWSbpa0hMRsV3SF5IG+f2gSzXzf+ZXSNosaaPt28tO1b6awt1oKfGgsL1eM9F+JiLGSs9T0HWSbrH9vmYun91g++myIxV1QtKJiJj7P7CDmgn5oLpR0nsR0YuI05LGJF1beKbW1RRulhLPsm3NXMM8GhGPlp6npIjYGxFbI2JUM8+JVyJizf9EtZyI+EjSh7bnthTvkPR2wZFKOy7pGtsbZl83OzQAb9auuCz4QmEp8Tmuk3SHpCO235y97aGIeLHgTKjHnyU9M/sDzruS7iw8TzER8Zrtg5Le0MynsSY0AL9FyW9OAkAyNV0qAQA0QLgBIBnCDQDJEG4ASIZwA0AyhBsAkiHcAJAM4QaAZP4P1Pe5nO/mQa8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpp=dpp_plane(10,10,0.1)\n",
    "xy=dpp.sample(verbose=True)\n",
    "print(xy.shape)\n",
    "plt.scatter(xy[:,0],xy[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        This step differs from matlab code in the following way:\n",
    "            \n",
    "        1.  Create vector with valid indices which can be sampled\n",
    "        2.  Normalise the probabilities\n",
    "        3.  Make use of `np.random.choice` to choose (guarentees that it will be a new choice for `Y`)\n",
    "        \"\"\"        \n",
    "        P_index = [(indx, prob) for indx, prob in list(zip(range(len(P)), P)) if indx not in Y]\n",
    "        P_list = [x for x, _ in P_index]\n",
    "        P_norm = np.array([p for _, p in P_index])\n",
    "        P_norm = P_norm/np.sum(P_norm)\n",
    "        choose_item = np.random.choice(range(len(P_list)), 1, p=P_norm)[0]\n",
    "        \n",
    "        # add the index into our sampler\n",
    "        Y.append(y_index[choose_item])\n",
    "        if len(Y) == k:\n",
    "            return Y\n",
    "        \n",
    "        # delete item from y_index...\n",
    "        y_index.pop(choose_item)\n",
    "\n",
    "        # update...choose a vector to elinate, lets pick randomly\n",
    "        j = random.choice(range(V.shape[1]))\n",
    "        Vj = V[:, j]\n",
    "        V = np.delete(V, j, axis=1)\n",
    "        \n",
    "        # make sure we do a projection onto Vj, \n",
    "        # is orthogonal basis\n",
    "        V_norm = V[choose_item, :]/Vj[choose_item]\n",
    "        V = V - (Vj.reshape(-1, 1).dot(V_norm.reshape(1, -1))) + (np.ones(V.shape) * np.finfo(float).eps)\n",
    "        \n",
    "        # orthogonalise\n",
    "        for a in range(V.shape[1]):\n",
    "            for b in range(a):\n",
    "                V[:, a] = V[:, a] - ((V[:, a].T).dot(V[:, b]))*(V[:, b])\n",
    "            V[:, a] = V[:, a]/np.linalg.norm(V[:, a])\n",
    "\n",
    "    \n",
    "\n",
    "def decompose_kernel(M):\n",
    "    \"\"\"\n",
    "    Decomposes the kernel so that dpp function can sample.     \n",
    "    This function returns:\n",
    "        * M - the original kernel\n",
    "        * V - eigenvectors\n",
    "        * D - diagonals of eigenvalues\n",
    "    \"\"\"\n",
    "    L = {}    \n",
    "    D, V  = np.linalg.eig(M)\n",
    "    L['M'] = M.copy()\n",
    "    L['V'] = np.real(V.copy())\n",
    "    L['D'] = np.real(D.copy())\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(0,6)\n",
    "Y = np.arange(0,6)\n",
    "data = np.zeros((len(X)*len(Y), 2))\n",
    "n=0\n",
    "for i in range(0,len(X)):\n",
    "    for j in range(0,len(Y)):\n",
    "        data[n]=(X[i],Y[j])\n",
    "        n += 1\n",
    "print(data.shape)        \n",
    "        \n",
    "M= rbf_kernel(data,data,gamma=0.2)\n",
    "L = decompose_kernel(M)\n",
    "indx = sample_dpp(L=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L['D'])\n",
    "D = np.divide(L['D'], (1+L['D']))\n",
    "print(D)\n",
    "print(D.size)\n",
    "p = np.random.rand(D.size)\n",
    "\n",
    "\n",
    "#print(M.shape)\n",
    "#L = decompose_kernel(M)\n",
    "\n",
    "print(len(L['D']))\n",
    "        \n",
    "#sklearn.gaussian_process.kernels.RBF(length_scale=1.0, length_scale_boun\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X : array, shape (n_samples_X, n_features)\n",
    "Left argument of the returned kernel k(X, Y)\n",
    "\n",
    "Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n",
    "Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.\n",
    "\n",
    "eval_gradient : bool (optional, default=False)\n",
    "Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.\n",
    "\n",
    "Returns:\t\n",
    "K : array, shape (n_samples_X, n_samples_Y)\n",
    "Kernel k(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#M = rbf_kernel(iris.data.T)\n",
    "#L = decompose_kernel(M)\n",
    "#indx = sample_dpp(L=L, k=3)\n",
    "#print(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class kernel\n",
    "    def __init__(self,width=6,height=6,scale=2):\n",
    "        self.width =width\n",
    "        self.height =height\n",
    "        self.size = width*height\n",
    "        X = np.arange(0,self.width)\n",
    "        Y = np.arange(0,self.height)\n",
    "        for \n",
    "        \n",
    "        self.M = np.array([self.size,self.size])\n",
    "        for i in width\n",
    "        \n",
    "        ()\n",
    "\n",
    "\n",
    "X = np.linspace(-1.0, 1.0, num=6)\n",
    "Y = np.linspace(-1.0, 1.0, num=6)\n",
    "print(X)\n",
    "for i in range(0,len(X)):\n",
    "    print(X[i])\n",
    "\n",
    "    \n",
    "#X, Y = np.meshgrid(X, Y)\n",
    "#print(X,Y)\n",
    "\n",
    "#kernel = np.array([64,64],dtype=np.float)\n",
    "#kernel.size\n",
    "\n",
    "\n",
    "#\n",
    "#numpy.linalg.eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyro]",
   "language": "python",
   "name": "conda-env-pyro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
