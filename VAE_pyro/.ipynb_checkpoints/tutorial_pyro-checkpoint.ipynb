{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials in pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define a normal distribution\n",
    "loc = 0.   # mean zero\n",
    "scale = 1. # unit variance\n",
    "normal = dist.Normal(loc, scale) # create a normal distribution object\n",
    "\n",
    "# Sample from the distribution\n",
    "x = normal.sample() # draw a sample from N(0,1)\n",
    "print(\"sample\", x)\n",
    "\n",
    "# Ask what is the probability of a sample under that distribution\n",
    "print(\"log prob\", normal.log_prob(x)) # score the sample from N(0,1)\n",
    "\n",
    "# normal.log_prob can bu used to plot the log_PDF\n",
    "x=list()\n",
    "logp=list()\n",
    "for n in range(-100,100):\n",
    "    tmp=n*0.1\n",
    "    x.append(tmp)\n",
    "    logp.append(normal.log_prob(tmp).item())\n",
    "plt.plot(x,logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', dist.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', dist.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "\n",
    "def ice_cream_sales():\n",
    "    cloudy, temp = weather()\n",
    "    expected_sales = 200. if cloudy == 'sunny' and temp > 80.0 else 50.\n",
    "    ice_cream = pyro.sample('ice_cream', dist.Normal(expected_sales, 10.0))\n",
    "    return ice_cream.item(),cloudy, temp\n",
    "\n",
    "for _ in range(3):\n",
    "    print(weather())\n",
    "\n",
    "for _ in range(3):\n",
    "    print(ice_cream_sales())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedurally, weather() is a non-deterministic Python callable that returns two random samples. Because the randomness is invoked with pyro.sample, however, it is much more than that. In particular weather() specifies a joint probability distribution over two named random variables: cloudy and temp. As such, it defines a probabilistic model that we can reason about using the techniques of probability theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric(p, t=None):\n",
    "    if t is None:\n",
    "        t = 0\n",
    "    x = pyro.sample(\"x_{}\".format(t), dist.Bernoulli(p))\n",
    "    if x.item() == 0:\n",
    "        return x\n",
    "    else:\n",
    "        return x + geometric(p, t + 1)\n",
    "\n",
    "print(geometric(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_product(loc, scale):\n",
    "    z1 = pyro.sample(\"z1\", dist.Normal(loc, scale))\n",
    "    z2 = pyro.sample(\"z2\", dist.Normal(loc, scale))\n",
    "    y = z1 * z2\n",
    "    return y\n",
    "\n",
    "def make_normal_normal():\n",
    "    mu_latent = pyro.sample(\"mu_latent\", dist.Normal(0, 1))\n",
    "    fn = lambda scale: normal_product(mu_latent, scale)\n",
    "    print(type(fn)) # I have created a lambda function \n",
    "    # It takes a scale and returns normal_product(mu_latent, scale)\n",
    "    return fn\n",
    "\n",
    "print(make_normal_normal()(1.32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(guess):\n",
    "    # Prior \n",
    "    true_weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0)) \n",
    "    # Likelyhood  \n",
    "    measured_weight = pyro.sample(\"measurement\", dist.Normal(true_weight, 0.75)) \n",
    "    return measured_weight\n",
    "\n",
    "print(type(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force approach.\n",
    "# Check the behavior of my model.\n",
    "# I run the model forward for a given value of guess\n",
    "\n",
    "# The marginal distribution concentrates around the data\n",
    "std_list1=list()\n",
    "for _ in range(10):\n",
    "    exp1=list()\n",
    "    for _ in range(100):\n",
    "        exp1.append(scale(8).item())\n",
    "    print(\"mean -> \",np.mean(exp1),\"std-> \",np.std(exp1))\n",
    "    std_list1.append(np.std(exp1))\n",
    "\n",
    "print(\"average std=\",np.mean(std_list1))\n",
    "plt.hist(exp1, range=(3.0, 13.0))\n",
    "plt.title(\"P(measurement | guess=8) brute force\")\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"# of measurement\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is method based on inportance sampling\n",
    "posterior = pyro.infer.Importance(scale, num_samples=100)\n",
    "marginal = pyro.infer.EmpiricalMarginal(posterior.run(8.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The marginal distribution concentrates around the data\n",
    "std_list2=list()\n",
    "for _ in range(10):\n",
    "    exp2=list()\n",
    "    for _ in range(100):\n",
    "        exp2.append(marginal().item())\n",
    "    print(\"mean -> \",np.mean(exp2),\"std-> \",np.std(exp2))\n",
    "    std_list2.append(np.std(exp2))\n",
    "\n",
    "print(\"average std=\",np.mean(std_list2))\n",
    "plt.hist(exp2, range=(3.0, 13.0))\n",
    "plt.title(\"P(measurement | guess=8) smart way\")\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"# of measurement\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's nice but what if we want to plot \n",
    "# the posterior \"P(measurement | guess=8, data=20)\"\n",
    "\n",
    "conditioned_scale = pyro.condition(scale, data={\"measurement\": 20.0})\n",
    "\n",
    "print(type(conditioned_scale))\n",
    "\n",
    "posterior = pyro.infer.Importance(conditioned_scale, num_samples=100)\n",
    "marginal = pyro.infer.EmpiricalMarginal(posterior.run(8.0), sites=\"weight\")\n",
    "\n",
    "\n",
    "exp2=list()\n",
    "for _ in range(100):\n",
    "    exp2.append(marginal().item())\n",
    "    \n",
    "# The marginal distribution concentrates around the data\n",
    "print(np.mean(exp2))\n",
    "print(np.std(exp2))\n",
    "plt.hist(exp2, range=(3.0, 23.0))\n",
    "plt.title(\"P(measurement | guess=8, data=20.0)\")\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"# of measurement\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's nice but what if we want to plot \n",
    "# the posterior \"P(measurement | guess=8, data=[a,x,z,c,d,s])\"\n",
    "\n",
    "def scale_5(guess):\n",
    "    # Prior \n",
    "    true_weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0)) \n",
    "    # Likelyhood\n",
    "    m=list()\n",
    "    for i in range(5):\n",
    "        m.append(pyro.sample(\"m_\"+str(i), dist.Normal(true_weight, 0.75)).item()) \n",
    "    return m\n",
    "\n",
    "obs={\"m_0\" : 12, \"m_1\" : 13, \"m_2\" : 14, \"m_3\" : 15 ,\"m_4\" : 16 }\n",
    "\n",
    "conditioned_scale_5 = pyro.condition(scale_5, data=obs)\n",
    "\n",
    "posterior = pyro.infer.Importance(conditioned_scale, num_samples=100)\n",
    "marginal = pyro.infer.EmpiricalMarginal(posterior.run(8.0), sites=\"weight\")\n",
    "\n",
    "\n",
    "exp3=list()\n",
    "for _ in range(100):\n",
    "    exp3.append(marginal().item())\n",
    "    \n",
    "\n",
    "# The marginal distribution concentrates around the data\n",
    "print(np.mean(exp3))\n",
    "print(np.std(exp3))\n",
    "plt.hist(exp2, range=(3.0, 23.0))\n",
    "plt.title(\"P(measurement | guess=8, data=obs)\")\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"# of measurement\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could write out the exact posterior distribution for scale, in general it is intractable to specify a guide that is a good approximation to the posterior distribution of an arbitrary conditioned stochastic function. What we can do instead is use the top-level function pyro.param to specify a family of guides indexed by named parameters, and search for the member of that family that is the best approximation. This approach to approximate posterior inference is called variational inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyro]",
   "language": "python",
   "name": "conda-env-pyro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
