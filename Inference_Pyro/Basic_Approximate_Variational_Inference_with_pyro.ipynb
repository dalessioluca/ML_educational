{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of **BASIC APPROXIMATE VARIATIONAL INFERENCE** with pyro. It is the flip coin example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step, alpha, beta 0 80.12602233886719 191.89492797851562\n",
      "step, alpha, beta 100 11.850266456604004 138.74346923828125\n",
      "step, alpha, beta 200 5.099936008453369 49.65652847290039\n",
      "step, alpha, beta 300 3.085559606552124 26.45911407470703\n",
      "step, alpha, beta 400 2.096299409866333 26.560930252075195\n",
      "step, alpha, beta 500 1.639630913734436 16.459138870239258\n",
      "step, alpha, beta 600 2.1707279682159424 24.77992057800293\n",
      "step, alpha, beta 700 2.067006826400757 20.55565643310547\n",
      "step, alpha, beta 800 1.6133599281311035 24.301687240600586\n",
      "step, alpha, beta 900 1.7965360879898071 25.169696807861328\n",
      "step, alpha, beta 1000 2.129049301147461 20.82669448852539\n",
      "step, alpha, beta 1100 1.882346510887146 23.698848724365234\n",
      "step, alpha, beta 1200 2.3660542964935303 23.771549224853516\n",
      "step, alpha, beta 1300 1.8733506202697754 20.857290267944336\n",
      "step, alpha, beta 1400 1.8274405002593994 20.030136108398438\n",
      "step, alpha, beta 1500 2.1322097778320312 22.286333084106445\n",
      "step, alpha, beta 1600 1.7033658027648926 18.909765243530273\n",
      "step, alpha, beta 1700 1.482367753982544 14.35377025604248\n",
      "step, alpha, beta 1800 2.381746768951416 13.025510787963867\n",
      "step, alpha, beta 1900 2.042922258377075 20.48955726623535\n",
      "\n",
      "\n",
      "Inferred alpha_q, beta_q = 2.139111042022705 19.989065170288086\n",
      "Exact    alpha ,  beta   = 2.0 19.0\n",
      "Inferred fairness= 0.097 +- 0.061\n",
      "Exact    fairness= 0.095 +- 0.063\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "from pyro.optim import Adam, SGD\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "n_steps = 2 if smoke_test else 2000\n",
    "\n",
    "# enable validation (e.g. validate parameters of distributions)\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# create some data with 1 observed heads and 9 observed tails\n",
    "data = []\n",
    "for _ in range(1):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(9):\n",
    "    data.append(torch.tensor(0.0))\n",
    "    \n",
    "## WOW. In python everything defined outside a function is considered global \n",
    "## and can be used inside the function as well    \n",
    "prior_dict = { \"alpha_0\": torch.tensor(1.0), \"beta_0\" : torch.tensor(10.0)}\n",
    "    \n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = prior_dict[\"alpha_0\"]\n",
    "    beta0  = prior_dict[\"beta_0\"]\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(125.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(125.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "# setup the optimizer\n",
    "#adam_params = {\"lr\": 0.05, \"betas\": (0.90, 0.999)}\n",
    "#optimizer = Adam(adam_params)\n",
    "sgd_params = {\"lr\": 0.05}\n",
    "optimizer = SGD(sgd_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "    if step % 100 == 0:\n",
    "        print(\"step, alpha, beta\",step,pyro.param(\"alpha_q\").item(),pyro.param(\"beta_q\").item())\n",
    "\n",
    "# Exact values can be inferred due to conjugancy between Beta (prior dist) and Bernully (likelyhood)\n",
    "observed_head = sum(data).item()\n",
    "observed_tail = len(data) - observed_head\n",
    "alpha_exact = prior_dict[\"alpha_0\"].item() + observed_head\n",
    "beta_exact  = prior_dict[\"beta_0\"].item()  + observed_tail\n",
    "\n",
    "def compute_mean_var_of_beta_dist(a,b):\n",
    "    \"\"\" Return the mean and std of Beta distribution \n",
    "        when the parameters alpha,beta are given\n",
    "    \"\"\"\n",
    "    mean = a/(a+b)\n",
    "    factor = b/(a*(1+a+b))\n",
    "    std = mean * math.sqrt(factor)\n",
    "    return mean,std\n",
    "\n",
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q  = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# print the result\n",
    "print(\"\\n\")\n",
    "print(\"Inferred alpha_q, beta_q =\",alpha_q,beta_q)\n",
    "print(\"Exact    alpha ,  beta   =\",alpha_exact,beta_exact)\n",
    "print(\"Inferred fairness= %.3f +- %.3f\" % compute_mean_var_of_beta_dist(alpha_q,beta_q))\n",
    "print(\"Exact    fairness= %.3f +- %.3f\" % compute_mean_var_of_beta_dist(alpha_exact,beta_exact))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyro]",
   "language": "python",
   "name": "conda-env-pyro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
